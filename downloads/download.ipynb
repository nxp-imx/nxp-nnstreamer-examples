{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bda40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "# Copyright 2022-2023, 2025 NXP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853551b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Models download\n",
    "Purpose of this notebook is to download model sources and metadata used by the examples, and apply necessary conversion steps prior to execution. Resulting models will be located in the top level `downloads` directory.\n",
    "\n",
    "This notebook has dependency on availability of eIQ Toolkit environment being configured.\n",
    "Refer to [downloads\\README.md](./README.md) for instructions.\n",
    "\n",
    "## Preamble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3f0df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import tarfile\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "# Repository directory\n",
    "top_dir = str(cwd.parent.absolute())\n",
    "# Models directory\n",
    "models_dir = os.path.join(top_dir, 'downloads', 'models')\n",
    "# Media directory\n",
    "media_dir = os.path.join(top_dir, 'downloads', 'media')\n",
    "# Work directory (temporary)\n",
    "tmp_dir = os.path.join(top_dir, 'tmp')\n",
    "# Small batch of pictures samples for post-training quantization\n",
    "samples_dir = os.path.join(top_dir, 'samples')\n",
    "# Neutron converter binary\n",
    "bsp_release = \"MCU_SDK_25.09.00+Linux_6.12.34_2.1.0\"\n",
    "neutron_converter_path = os.path.join(os.getenv('EIQ_BASE'), f'bin/neutron-converter/{bsp_release}/neutron-converter')\n",
    "\n",
    "# notebook dependency check:\n",
    "# eIQ environment verification\n",
    "try:\n",
    "    cmd = [neutron_converter_path, '--version']\n",
    "    subprocess.run(cmd, check=True, stdout = subprocess.DEVNULL)\n",
    "except:\n",
    "    raise FileNotFoundError('Need eIQ Toolkit environment setup - see downloads/README.md')\n",
    "\n",
    "# \n",
    "# General purpose helpers\n",
    "#\n",
    "\n",
    "# Download a list of files from urls\n",
    "# returns list of local file paths\n",
    "def fetch_urls(urls, dest_dir):\n",
    "    Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "    files = []\n",
    "    for url in urls:\n",
    "        path = urllib.parse.urlparse(url).path\n",
    "        file = os.path.basename(path)\n",
    "        dest_file = os.path.join(dest_dir, file)\n",
    "        print(f\"downloading {url} ({dest_file})\")\n",
    "        # wikimedia.org requires user agent\n",
    "        req = urllib.request.Request(\n",
    "                  url, data=None,\n",
    "                  headers= {'User-Agent' : 'download script/0.1'}\n",
    "              )\n",
    "        with urllib.request.urlopen(req) as resp:\n",
    "            with open(dest_file, 'wb') as dest:\n",
    "                content = resp.read()\n",
    "                dest.write(content)\n",
    "                files += [ dest_file ]\n",
    "    return files\n",
    "\n",
    "# download files and tarballs - extract archives when relevant\n",
    "def fetcher(items):\n",
    "    for item in items:\n",
    "        url = item['url']\n",
    "        dest = item['dest']\n",
    "        download = fetch_urls([url], dest)[0]\n",
    "        if download.endswith('.zip'):\n",
    "            with zipfile.ZipFile(download) as _zip:\n",
    "                print(f\"extracting zip {download}\")\n",
    "                _zip.extractall(path=dest)\n",
    "        if download.endswith('.tar.gz') or download.endswith('.tgz'):\n",
    "            with tarfile.open(download) as _tar:\n",
    "                print(f\"extracting tar {download}\")\n",
    "                _tar.extractall(path=dest)\n",
    "\n",
    "# Create list of clean (empty) directories\n",
    "def make_clean_dirs(dest_dirs):\n",
    "    for dest_dir in dest_dirs:\n",
    "        shutil.rmtree(dest_dir, ignore_errors=True)\n",
    "        Path(dest_dir).mkdir(parents=True)\n",
    "\n",
    "# Function to load and preprocess a single image\n",
    "def load_and_preprocess_image(path, input_shape):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_image(image, channels=input_shape[3])\n",
    "    image = tf.image.resize(image, [input_shape[1], input_shape[2]])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Representative dataset generator for TFLite quantization\n",
    "# Use the samples if provided, else generate random ones\n",
    "def representative_dataset_gen(input_shape, samples_dir=None):\n",
    "    if samples_dir and os.path.exists(samples_dir):\n",
    "        samples_paths = []\n",
    "        for root, dirs, files in os.walk(samples_dir):\n",
    "            for file in files:\n",
    "                # Construct absolute file path\n",
    "                absolute_path = os.path.abspath(os.path.join(root, file))\n",
    "                samples_paths.append(absolute_path)\n",
    "        for path in samples_paths:\n",
    "            image = load_and_preprocess_image(path, input_shape)\n",
    "            yield [image]\n",
    "    else:\n",
    "        for _ in range(100):\n",
    "            yield [np.random.rand(1, input_shape[1], input_shape[2], 3).astype(np.float32)]\n",
    "\n",
    "# Invoke TF Converter from frozen graph\n",
    "def convert_frozen_graph_to_tflite(origin, dest, input_name, output_name, input_shape,\n",
    "                                   quantize=False, input_type=False, output_type=False, samples_dir=False):\n",
    "    input_arrays = [input_name] if isinstance(input_name, str) else input_name\n",
    "    output_arrays = [output_name] if isinstance(output_name, str) else output_name\n",
    "\n",
    "    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
    "        graph_def_file=origin,\n",
    "        input_arrays=input_arrays,\n",
    "        output_arrays=output_arrays,\n",
    "        input_shapes={input_name: input_shape}\n",
    "    )\n",
    "    if quantize:\n",
    "        converter.post_training_quantize = True\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.inference_input_type = input_type\n",
    "        converter.inference_output_type = output_type\n",
    "        converter.representative_dataset = lambda: representative_dataset_gen(input_shape, samples_dir)\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    with open(dest, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Model converted and saved to: {dest}\")\n",
    "\n",
    "# Invoke neutron-converter with args\n",
    "def neutron_converter(origin, dest, args=['--target', 'imx95']):\n",
    "    cmd = [neutron_converter_path, '--input'] + [origin] + args + ['--output'] + [dest]\n",
    "    subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26e773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download a few jpeg picture samples for post-training quantization (for demo)\n",
    "shutil.rmtree(samples_dir, ignore_errors=True)    \n",
    "samples = [\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Olea_europaea_cuspidata-africana_Cape_Town.JPG/605px-Olea_europaea_cuspidata-africana_Cape_Town.JPG',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Sport-orange-red-color-tennis-organ-491989-pxhere.jpg/640px-Sport-orange-red-color-tennis-organ-491989-pxhere.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Domesticated_goose_head%2C_Chaguaramal%2C_Venezuela.jpg/640px-Domesticated_goose_head%2C_Chaguaramal%2C_Venezuela.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/20080831-R0012506.JPG/640px-20080831-R0012506.JPG',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Pisa_Cathedral_%26_Bell_Tower_-_%22Leaning_Tower_of_Pisa%22_%289809628764%29.jpg/640px-Pisa_Cathedral_%26_Bell_Tower_-_%22Leaning_Tower_of_Pisa%22_%289809628764%29.jpg',\n",
    "    ]\n",
    "fetch_urls(samples, samples_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b3d51",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Object Detection\n",
    "## Artifacts download\n",
    "\n",
    "| models | references | \n",
    "| :- | :- |\n",
    "| [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) | [TensorFlow 1 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models) |\n",
    "| [YoloV4-tiny](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights) | [Yolo Original Implementation in Darknet](https://github.com/AlexeyAB/darknet#pre-trained-models) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ec7a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.core.framework.graph_pb2 import GraphDef\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "def load_and_prune_graph(pb_path, output_nodes):\n",
    "    with tf.io.gfile.GFile(pb_path, \"rb\") as f:\n",
    "        graph_def = GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Keep only the subgraph needed to compute the outputs\n",
    "    pruned_graph_def = graph_util.extract_sub_graph(graph_def, output_nodes)\n",
    "    return pruned_graph_def\n",
    "\n",
    "\n",
    "def save_graph(graph_def, output_path):\n",
    "    with tf.io.gfile.GFile(output_path, \"wb\") as f:\n",
    "        f.write(graph_def.SerializeToString())\n",
    "\n",
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'object-detection')\n",
    "make_clean_dirs([tmp_dir, dest_dir])\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "model_float = 'ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz'\n",
    "url_float = urllib.parse.urljoin('http://download.tensorflow.org/models/object_detection/', model_float)\n",
    "packages = [\n",
    "    {'url'  : url_float,\n",
    "     'dest' : tmp_dir},\n",
    "    {'url'  : 'https://github.com/nnsuite/testcases/raw/master/DeepLearningModels/tensorflow-lite/ssd_mobilenet_v2_coco/coco_labels_list.txt',\n",
    "     'dest' : dest_dir},\n",
    "    {'url'  : 'https://github.com/nnsuite/testcases/raw/master/DeepLearningModels/tensorflow-lite/ssd_mobilenet_v2_coco/box_priors.txt',\n",
    "     'dest' : dest_dir},\n",
    "]\n",
    "\n",
    "fetcher(packages)\n",
    "\n",
    "folder = re.sub('.tar.gz', '', model_float)\n",
    "pb_model = os.path.join(tmp_dir, folder, 'frozen_inference_graph.pb')\n",
    "\n",
    "model_darknet = 'yolov4-tiny.weights'\n",
    "url_darknet = urllib.parse.urljoin('https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights', model_darknet)\n",
    "packages = [\n",
    "    {'url'  : url_darknet,\n",
    "     'dest' : tmp_dir},\n",
    "    {'url'  : 'https://raw.githubusercontent.com/amikelive/coco-labels/master/coco-labels-2014_2017.txt',\n",
    "     'dest' : dest_dir},\n",
    "]\n",
    "\n",
    "fetcher(packages)\n",
    "\n",
    "# Frozen graph for SSD MobileNet v2 contains Control Flow V1 ops (like tf.cond, tf.while_loop) as part of the postprocessing step.\n",
    "# TFLiteConverter cannot handle complex control flow logic from older TensorFlow 1.x-style graphs unless it's rewritten using\n",
    "# Control Flow V2 â€” possible only when having access to the original model code.\n",
    "# Due to the fact that postprocessing step is not needed, raw output tensors (concat, concat_1) can be used as output nodes,\n",
    "# thus graph can be pruned manually to remove everything after.\n",
    "output_nodes = ['concat', 'concat_1']\n",
    "pruned_graph = load_and_prune_graph(pb_model, output_nodes)\n",
    "pruned_pb_model = os.path.join(tmp_dir, folder, 'pruned_model.pb')\n",
    "save_graph(pruned_graph, pruned_pb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd1581",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Models conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c384c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TFLite models (without box-decoding / NMS postprocessing)\n",
    "#\n",
    "\n",
    "# convert TF -> TFLite (float)\n",
    "tflite_float = os.path.join(dest_dir, 'ssdlite_mobilenet_v2_coco_no_postprocess.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pruned_pb_model,\n",
    "    dest=tflite_float,\n",
    "    input_name='Preprocessor/sub',\n",
    "    output_name=['concat', 'concat_1'],\n",
    "    input_shape=[1, 300, 300, 3])\n",
    "\n",
    "# convert TF -> TFLite (quantized)\n",
    "tflite_quant = os.path.join(dest_dir, 'ssdlite_mobilenet_v2_coco_quant_uint8_float32_no_postprocess.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pruned_pb_model,\n",
    "    dest=tflite_quant,\n",
    "    input_name='Preprocessor/sub',\n",
    "    output_name=['concat', 'concat_1'],\n",
    "    input_shape=[1, 300, 300, 3],\n",
    "    quantize=True,\n",
    "    input_type=tf.uint8,\n",
    "    output_type=tf.float32)\n",
    "\n",
    "# convert TFLite (quantized) -> TFLite (neutron)\n",
    "tflite_neutron = os.path.join(dest_dir, 'ssdlite_mobilenet_v2_coco_quant_uint8_float32_no_postprocess_neutron.tflite')\n",
    "neutron_converter(tflite_quant, tflite_neutron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5047916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert Yolov4-tiny Darknet -> TFlite (quantized)\n",
    "# this script may take several minutes to complete\n",
    "!python3 {top_dir}/tasks/object-detection/yolov4-tiny_export_model.py \\\n",
    "--weights_path={tmp_dir} --output_path={dest_dir} --images_path={samples_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95146a8d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#  Classification\n",
    "## Artifacts download\n",
    "| model | reference | \n",
    "| :- | :- |\n",
    "| [mobilenet_v1_1.0_224](http://download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz) | [TensorFlow-Slim image classification model library](https://github.com/tensorflow/models/tree/master/research/slim) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e6e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'classification')\n",
    "make_clean_dirs([tmp_dir, dest_dir])\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "model_float = 'mobilenet_v1_1.0_224.tgz'\n",
    "url_model = 'http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/'\n",
    "packages = [\n",
    "    {'url'  : urllib.parse.urljoin(url_model, model_float),\n",
    "     'dest' : tmp_dir},\n",
    "    {'url'  : 'https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip',\n",
    "     'dest' : tmp_dir},\n",
    "]\n",
    "\n",
    "fetcher(packages)\n",
    "\n",
    "pb_model_float = os.path.join(tmp_dir, 'mobilenet_v1_1.0_224_frozen.pb')\n",
    "\n",
    "labels_file = os.path.join(tmp_dir, 'labels_mobilenet_quant_v1_224.txt')\n",
    "shutil.copy2(labels_file, dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80733e7b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Models conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3307ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TFLite models\n",
    "#\n",
    "\n",
    "# convert TF -> TFLite (float)\n",
    "tflite_float = os.path.join(dest_dir, 'mobilenet_v1_1.0_224.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pb_model_float,\n",
    "    dest=tflite_float,\n",
    "    input_name='input',\n",
    "    output_name='MobilenetV1/Predictions/Reshape_1',\n",
    "    input_shape=[1, 224, 224, 3])\n",
    "\n",
    "# convert TF -> TFLite (quantized)\n",
    "tflite_quant = os.path.join(dest_dir, 'mobilenet_v1_1.0_224_quant_uint8_float32.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pb_model_float,\n",
    "    dest=tflite_quant,\n",
    "    input_name='input',\n",
    "    output_name='MobilenetV1/Predictions/Reshape_1',\n",
    "    input_shape=[1, 224, 224, 3],\n",
    "    quantize=True,\n",
    "    input_type=tf.uint8,\n",
    "    output_type=tf.float32,\n",
    "    samples_dir=samples_dir)\n",
    "\n",
    "# convert TFLite (quantized) -> TFLite (neutron)\n",
    "tflite_neutron = os.path.join(dest_dir, 'mobilenet_v1_1.0_224_quant_uint8_float32_neutron.tflite')\n",
    "neutron_converter(tflite_quant, tflite_neutron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a17ac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Semantic Segmentation\n",
    "## Artifacts download\n",
    "| model | references | \n",
    "| :- | :- |\n",
    "| [deeplabv3_mnv2_dm05](http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz) | [TensorFlow DeepLab Model Zoo](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md) <br> [Quantize DeepLab model for faster on-device inference](https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/quantize.md) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85aea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'semantic-segmentation')\n",
    "images_dir = os.path.join(media_dir, 'pascal_voc_2012_images')\n",
    "make_clean_dirs([tmp_dir, dest_dir, images_dir])\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "model_float = 'deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz'\n",
    "url_model = 'http://download.tensorflow.org/models/'\n",
    "packages = [\n",
    "    {'url'  : urllib.parse.urljoin(url_model, model_float),\n",
    "     'dest' : tmp_dir},\n",
    "]\n",
    "\n",
    "fetcher(packages)\n",
    "\n",
    "pb_model_float = os.path.join(tmp_dir, 'deeplabv3_mnv2_dm05_pascal_trainaug','frozen_inference_graph.pb')\n",
    "\n",
    "# fetch and rename a few images relevant to pascal voc 2012 data set\n",
    "images = [\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/a/ac/Plectranthus_verticillatus_in_a_ceramic_pot.jpg/640px-Plectranthus_verticillatus_in_a_ceramic_pot.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/Maceta.jpg/566px-Maceta.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/f/f6/Stray_Cat%2C_Nafplio.jpg/800px-Stray_Cat%2C_Nafplio.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Feral_cat_7.jpg/800px-Feral_cat_7.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/0/0a/Senior-lady-dog-walker.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Texel_ewe_and_three_lambs.jpg/800px-Texel_ewe_and_three_lambs.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Cow_farm_in_Bemmel%2C_Lingewaard.jpg/800px-Cow_farm_in_Bemmel%2C_Lingewaard.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Urban_cycling_III.jpg/800px-Urban_cycling_III.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/LomondRoadsTTRider.jpg/467px-LomondRoadsTTRider.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/9/9c/Interceptor_of_Bangalore_Traffic_Police.jpg/640px-Interceptor_of_Bangalore_Traffic_Police.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c0/Ranger_clearing_traffic_at_a_bison_jam_in_Lamar_Valley_%2848fe8fe3-c1e3-4828-acec-496eaef43503%29.jpg/800px-Ranger_clearing_traffic_at_a_bison_jam_in_Lamar_Valley_%2848fe8fe3-c1e3-4828-acec-496eaef43503%29.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Finnair.a320-200.oh-lxf.arp.jpg/800px-Finnair.a320-200.oh-lxf.arp.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/G-BGMP_Reims_F172_%40Cotswold_Airport%2C_July_2005.jpg/800px-G-BGMP_Reims_F172_%40Cotswold_Airport%2C_July_2005.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/JNR_14_series_sleeper_Blue_Train_Hayabusa.jpg/640px-JNR_14_series_sleeper_Blue_Train_Hayabusa.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/7122_series_train_%2812%29.JPG/640px-7122_series_train_%2812%29.JPG',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Blue_fishing_boat_harbour_Eretria_Euboea_Greece.jpg/611px-Blue_fishing_boat_harbour_Eretria_Euboea_Greece.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/KYOEI_boat_in_Kisarazu_port_3.jpg/640px-KYOEI_boat_in_Kisarazu_port_3.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Jarritos_glass_bottle_%28Mexico%29.jpg/450px-Jarritos_glass_bottle_%28Mexico%29.jpg',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Andechser_3_beers.JPG/798px-Andechser_3_beers.JPG',\n",
    "    'https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/Ski_Citrus_Soda_%2825395355114%29.jpg/640px-Ski_Citrus_Soda_%2825395355114%29.jpg',\n",
    "]\n",
    "files = fetch_urls(images, tmp_dir)\n",
    "count = 0\n",
    "# rename files name as <image%04d.jpg>\n",
    "for src in files:\n",
    "    dest = os.path.join(images_dir, f'image{count:04}.jpg')\n",
    "    count += 1\n",
    "    shutil.copy2(src, dest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7fc2f0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Models conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f220698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# TFLite models\n",
    "#\n",
    "\n",
    "# convert TF -> TFLite (float)\n",
    "tflite_float = os.path.join(dest_dir, 'deeplabv3_mnv2_dm05_pascal.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pb_model_float,\n",
    "    dest=tflite_float,\n",
    "    input_name='MobilenetV2/MobilenetV2/input',\n",
    "    output_name='ResizeBilinear_2',\n",
    "    input_shape=[1, 513, 513, 3])\n",
    "\n",
    "# convert TF -> TFLite (quantized)\n",
    "tflite_quant = os.path.join(dest_dir, 'deeplabv3_mnv2_dm05_pascal_quant_uint8_float32.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pb_model_float,\n",
    "    dest=tflite_quant,\n",
    "    input_name='MobilenetV2/MobilenetV2/input',\n",
    "    output_name='ResizeBilinear_2',\n",
    "    input_shape=[1, 513, 513, 3],\n",
    "    quantize=True,\n",
    "    input_type=tf.uint8,\n",
    "    output_type=tf.float32,\n",
    "    samples_dir=samples_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c9353",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Pose Estimation\n",
    "## Artifacts download\n",
    "| model | references | \n",
    "| :- | :- |\n",
    "| [movenet/singlepose/lightning TF2 SavedModel](https://tfhub.dev/google/movenet/singlepose/lightning/4) | [TensorFlow Hub Image Pose Detection](https://tfhub.dev/s?module-type=image-pose-detection) <br> [TensorFlow Pose](https://www.tensorflow.org/lite/examples/pose_estimation/overview) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf94be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'pose-estimation')\n",
    "make_clean_dirs([tmp_dir, dest_dir])\n",
    "movies_dir = os.path.join(media_dir, 'movies')\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "\n",
    "# Download movie\n",
    "url_movie = 'https://upload.wikimedia.org/wikipedia/commons/transcoded/1/17/Conditioning_Drill_1-_Power_Jump.webm/Conditioning_Drill_1-_Power_Jump.webm.480p.vp9.webm'\n",
    "packages = [{'url': url_movie, 'dest': movies_dir},]\n",
    "fetcher(packages)\n",
    "\n",
    "# quantized model to be downloaded from network\n",
    "\n",
    "url_models = 'https://github.com/NXP/nxp-vision-model-zoo/releases/download/v1.1/nxp_eiq_vision_model_zoo_v1.1.tgz'\n",
    "packages = [{'url' :url_models, 'dest': tmp_dir},]\n",
    "fetcher(packages)\n",
    "\n",
    "models_tmp_dir = os.path.join(tmp_dir, 'tasks')\n",
    "\n",
    "!cp {models_tmp_dir}/pose-estimation/movenet/movenet.tflite \\\n",
    "{dest_dir}/movenet_quant.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc0153",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Face Processing\n",
    "## Artifacts download\n",
    "| models | references | \n",
    "| :- | :- |\n",
    "| [FaceNet512](https://github.com/serengil/deepface_models/releases/download/v1.0/facenet512_weights.h5) | [Deepface repository](https://github.com/serengil/deepface/tree/master/deepface/basemodels) |\n",
    "| [Ultraface-slim](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/models/pretrained/version-slim-320.pth) | [Ultra-Light-Fast-Generic-Face-Detector repository](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) |\n",
    "|[Deepface-emotion](https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5) | [Deepface repository](https://github.com/serengil/deepface/blob/master/deepface/extendedmodels/Emotion.py) |\n",
    "\n",
    "These models are downloaded from [NXP vision model zoo](https://github.com/NXP/nxp-vision-model-zoo) where they have been quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406558e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'face-processing')\n",
    "make_clean_dirs([tmp_dir, dest_dir])\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "\n",
    "url_models = 'https://github.com/NXP/nxp-vision-model-zoo/releases/download/v1.0/nxp_eiq_vision_model_zoo_v1.0.tgz'\n",
    "packages = [{'url' :url_models, 'dest': tmp_dir},]\n",
    "fetcher(packages)\n",
    "\n",
    "models_tmp_dir = os.path.join(tmp_dir, 'tasks')\n",
    "\n",
    "!cp {models_tmp_dir}/object-detection/ultraface-slim/ultraface_slim_uint8_float32.tflite \\\n",
    "{models_tmp_dir}/face-recognition/facenet512/facenet512_uint8.tflite \\\n",
    "{models_tmp_dir}/classification/deepface-emotion/emotion_uint8_float32.tflite \\\n",
    "{dest_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f88c3a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db1a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert TFLite (quantized) -> TFLite (neutron)\n",
    "tflite_quant = os.path.join(models_tmp_dir, 'object-detection/ultraface-slim/ultraface_slim_uint8_float32.tflite')\n",
    "tflite_neutron = os.path.join(dest_dir, 'ultraface_slim_uint8_float32_neutron.tflite')\n",
    "neutron_converter(tflite_quant, tflite_neutron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4560f3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Monocular Depth Estimation\n",
    "## Artifacts download\n",
    "| models | references | \n",
    "| :- | :- |\n",
    "| Midasv2 | [Midas repository](https://www.kaggle.com/models/intel/midas/tfLite/v2-1-small-lite/1) |\n",
    "\n",
    "The model is downloaded from kaggle, its performances and quantization process are available on [NXP vision model zoo](https://github.com/NXP/nxp-vision-model-zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14006c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start from clean state\n",
    "dest_dir = os.path.join(models_dir, 'monocular-depth-estimation')\n",
    "make_clean_dirs([tmp_dir, dest_dir])\n",
    "\n",
    "# models and metadata to be downloaded from network\n",
    "\n",
    "model_float = os.path.join(dest_dir, '')\n",
    "url_models = 'https://www.kaggle.com/api/v1/models/intel/midas/tfLite/v2-1-small-lite/1/download/1.tflite'\n",
    "packages = [{'url' :url_models, 'dest': dest_dir},]\n",
    "fetcher(packages)\n",
    "\n",
    "model_float = os.path.join(dest_dir, '1.tflite')\n",
    "renamed_model_float = os.path.join(dest_dir, 'midas_2_1_small_float32.tflite')\n",
    "os.rename(model_float, renamed_model_float)\n",
    "\n",
    "!cp {renamed_model_float} {tmp_dir}/midas_2_1_small_float32.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d0356",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Model conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fbd72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(tmp_dir)\n",
    "\n",
    "# convert TFLite -> TF for quantization process\n",
    "req_dir = os.path.join(top_dir, 'tasks/monocular-depth-estimation/requirements.txt')\n",
    "script_dir = os.path.join(top_dir, 'tasks/monocular-depth-estimation/export_midas-v2_to_TensorFlow.sh')\n",
    "os.system('chmod a+x ' + script_dir)\n",
    "os.system('bash ' + script_dir + ' ' + req_dir)\n",
    "\n",
    "# convert TF -> TFLite (quantized)\n",
    "pb_model = os.path.join(tmp_dir, 'model_float32.pb')\n",
    "tflite_quant = os.path.join(dest_dir, 'midas_2_1_small_int8_quant.tflite')\n",
    "convert_frozen_graph_to_tflite(\n",
    "    origin=pb_model,\n",
    "    dest=tflite_quant,\n",
    "    input_name='Const',\n",
    "    output_name='midas_net_custom/sequential/re_lu_9/Relu',\n",
    "    input_shape=[1, 256, 256, 3],\n",
    "    quantize=True,\n",
    "    input_type=tf.uint8,\n",
    "    output_type=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae5294",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Postamble / cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299cadc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove temporary directories\n",
    "shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "shutil.rmtree(samples_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
